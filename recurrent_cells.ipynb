{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvRNNCell(nn.Module):\n",
    "    def __init__(self, input_c, hidden_size, kernel_size, bias):\n",
    "        super(ConvRNNCell, self).__init__()\n",
    "        self.input_c = input_c\n",
    "        self.hidden_size = hidden_size\n",
    "        self.conv = nn.Conv2d(in_channels=input_c+hidden_size,out_channels=hidden_size,kernel_size=kernel_size,padding=kernel_size//2,bias=bias)\n",
    "        \n",
    "    def forward(self,x,h_cur):\n",
    "        (m,nc,nh,nw) = x.size()\n",
    "        if h_cur is None:\n",
    "            h_cur= Variable(torch.zeros(1, self.hidden_size, nh, nw)).to(device).float()\n",
    "\n",
    "        \n",
    "        combined = torch.cat((x, h_cur), dim=1)  # concatenate along channel axis\n",
    "        combined_conv = self.conv(combined)\n",
    "        h_next = F.tanh(combined_conv)\n",
    "\n",
    "        return h_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_c, hidden_size, kernel_size, bias):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        self.input_c = input_c\n",
    "        self.hidden_size = hidden_size\n",
    "        self.conv = nn.Conv2d(in_channels=input_c+hidden_size,out_channels=4*hidden_size,kernel_size=kernel_size,padding=kernel_size//2,bias=bias)\n",
    "        \n",
    "    def forward(self,x,cur_state):\n",
    "        (m,nc,nh,nw) = x.size()\n",
    "        if cur_state is None:\n",
    "            cur_state = (Variable(torch.zeros(1, self.hidden_size, nh, nw)).to(device).float(),\n",
    "                        Variable(torch.zeros(1, self.hidden_size, nh, nw)).to(device).float())\n",
    "        \n",
    "            \n",
    "        h_cur, c_cur = cur_state\n",
    "        combined = torch.cat((x, h_cur), dim=1)  # concatenate along channel axis\n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = combined_conv.chunk(4,1)\n",
    "\n",
    "        i = F.sigmoid(cc_i)\n",
    "        f = F.sigmoid(cc_f)\n",
    "        o = F.sigmoid(cc_o)\n",
    "        g = F.tanh(cc_g)\n",
    "\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * F.tanh(c_next)\n",
    "\n",
    "        next_state = (h_next,c_next)\n",
    "        return next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, flatten_dim, hidden_size, bias):\n",
    "        super(LSTMCell, self).__init__()\n",
    "    \n",
    "        self.flatten_dim = flatten_dim\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.bias        = bias\n",
    "        \n",
    "        self.i2h = nn.Linear(flatten_dim, 4*hidden_size, bias=bias)\n",
    "        self.h2h = nn.Linear(hidden_size, 4*hidden_size, bias=bias)\n",
    "        \n",
    " \n",
    "        \n",
    "    def forward(self, x, cur_state):\n",
    "        \n",
    "        if cur_state is None:\n",
    "            cur_state = (Variable(torch.zeros(1, self.hidden_size)).to(device).float(),\n",
    "                Variable(torch.zeros(1, self.hidden_size)).to(device).float())\n",
    "\n",
    "        x = x.view(1,-1)\n",
    "        c_cur, h_cur = cur_state\n",
    "        \n",
    "        preact = self.i2h(x) + self.h2h(h_cur)\n",
    "        #print(preact.size())\n",
    "        ingate, forgetgate, cellgate, outgate = preact.chunk(4, 1)\n",
    "        \n",
    "        \n",
    "        ingate = F.sigmoid(ingate)\n",
    "        forgetgate = F.sigmoid(forgetgate)\n",
    "        cellgate = F.tanh(cellgate)\n",
    "        outgate = F.sigmoid(outgate)\n",
    "        \n",
    "        c_next = (forgetgate * c_cur) + (ingate * cellgate)\n",
    "        h_next = outgate * F.tanh(c_next)\n",
    "        \n",
    "        next_state = (c_next,h_next)\n",
    "        \n",
    "        #softmax_out = F.softmax(self.linear(h_next.view(1,-1)),dim=1)\n",
    "        \n",
    "        return next_state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(1,512,7,7).to(device).float()\n",
    "\n",
    "rn = ConvRNNCell(512,128,3,True).to(device).float()\n",
    "h = rn(a,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, rn.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = ConvLSTMCell(512,128,3,True).to(device).float()\n",
    "model_parameters = filter(lambda p: p.requires_grad, ls.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
